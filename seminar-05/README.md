# Shared memory, ILP и другие

## Многомерные ядра и блоки

Как вы помните с прошлого семинара, для отправки задач на видеокарту CUDA создаёт блоки подзадач, каждая из которых выполняется на микроядре SM процессора. В специальных параметрах запуска мы определяли, сколько блоков будем создавать (В Grid'e) и сколько подзадач в каждом блоке. И так же упоминали, что возможно ещё делать и Grid и Блоки многомерными.

Так вот, сейчас и испробуем эту самую многомерность. И конечно же на матрицах.

Для начала напишем вспомогательный код для матриц

```c++
#include <iostream>

#define BLOCK_SIZE 256


void FillMatrix(float* matrix, int height, int width) {
	for (int i = 0; i < height; ++i) {
		for (int j = 0; j < width; ++j) {
			if (i == j) {
				matrix[i * width + j] = 1;
			} else {
				matrix[i * width + j] = 0;
			}
		}
	}
}

void PrintMatrix(float *matrix, int height, int width) {

	for (int i = 0; i < height; ++i) {
		for (int j = 0; j < width; ++j) {
			std::cout << i << " " << j << " " << matrix[i * width + j] << "\n";
		}
	}
}
```
В краце - первая генератор диагональных матриц. Вторая - просто печать матрицы по координатам

Далее, для создания многомерных блоков или гридов используется специальная переменная `dim3`. Тогда объявления 3-х мерных (или 2-х мерных в нашем случае) грида и блока будет примерно таким

```c++
dim3 num_blocks(8, 16);
dim3 block_size(16, 16);
```

Где в скобочках указаны размеры и блока и грида. Внимание, для блока всё так же действует ограничение, что суммарное количество потоков  в нём не превышает 1024. То есть не получится создать блок размера 32\*32\*32, так как это 32'768 попугаев. Кстати, третья размерность так же не может быть больше 64. Получаем, что **максимальные размерности блока** (не путать с размером!) 1024\*1024\*64.

Запишем целиком получившийся main

```c++
int main() {

	float *h_A;
	float *h_B;
	float *h_C;

	h_A = new float[128 * 384];
	h_B = new float[384 * 256];
	h_C = new float[128 * 256];

	FillMatrix(h_A, 128, 384);
	FillMatrix(h_B, 384, 256);

	float* d_A;
	float* d_B;
	float* d_C;

	cudaMalloc(&d_A, sizeof(float) * 128 * 384);
	cudaMalloc(&d_B, sizeof(float) * 384 * 256);
	cudaMalloc(&d_C, sizeof(float) * 128 * 256);

    cudaMemcpy(d_A, h_A, sizeof(float) * 128 * 384, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, sizeof(float) * 384 * 256, cudaMemcpyHostToDevice);

    // kernel call
    dim3 num_blocks(8, 16);
    dim3 block_size(16, 16);

    MatrixMul<<<num_blocks, block_size>>>(d_A, d_B, d_C, 384);

    cudaMemcpy(h_C, d_C, sizeof(float) * 128 * 256, cudaMemcpyDeviceToHost);
    PrintMatrix(h_C, 128, 256);

	cudaFree(d_A);
	cudaFree(d_B);
	cudaFree(d_C);

	delete[] h_A;
	delete[] h_B;
	delete[] h_C;

	return 0;
}
```

Сразу по коду - **массивы одномерны**! То есть не надо делать массив массивов, это другое. Да и не получится так. В остальном всё как обычно, просто запускаемся от *num_blocks* и *block_size* и работаем как обычно.

Теперь самое интересное - ядро. Так как у нас теперь всё двумерное, то можно этим воспользоваться для удобной "навигации" по матрице. Смотрите. Размер матрицы С 128\*256, а размеры блока и грида 8\*16 и 16\*16 соответственно. Нехитрыми догадками пятиклассника получаем:

```
8*16 = 128
16*16 = 256
```

Как раз по размеру матрицы. То есть мы можем просто каждую подзадачу в каждом блоке выделить на одну ячейку матрицы. Делаем!

```c++
__global__
void MatrixMul(float* A, float* B, float* C, int mid_size) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;

    int height = blockDim.x * gridDim.x;
    int width = blockDim.y * gridDim.y;

    C[i * width + j] = .0f;

    for (int k = 0; k < mid_size; ++k) {
        C[i * width + j] += A[i * mid_size + k] * B[k * width + j];
    }
}
```

Координаты i,j - это положение блока в матрице плюс положение подзадачи внутри блока. Тем самым i,j являются настоящими координатами элемента, который требуется обработать в матрице. Глянем на картинке

![Matrix](pic/matrix.png)

Слева оригинальная единичная матрица. Справа её разбиение блоками размера 2\*2. Тем самым мы получили грид 5\*5. То есть мы определили размер грида и блока так, чтобы блоки покрыли ровно всю матрицу.

И тогда элемент, который обрабатывается потоком номер (1,2) в блоке номер (2,2) будет

```
i = 2*blockDim + 2
j = 2*blockDim + 1
```

То есть сначала сдвигаемся по блокам (с шагом blockDim), а потом внутри блока по элементам, выбирая нужный.

Исходный код целиком лежит [здесь](https://github.com/akhtyamovpavel/ParallelComputationExamples/blob/master/CUDA/03.5-matrix-multiplication-example/main.cu)

## ILP

В прошлом семинаре мы обсудили, как должна строиться работа с оперативной памятью GPU. Напомню - микропроцессоры получают данные партиями по варпам - то есть каждый варп (32 микропроцессора) получает подряд идущие данные из массива (a[i+1], a[i+2], ..., a[i+32]). По этой причине приходилось особым образом оптимизировать работу с данными, чтобы потоки действительно обращались к данным, которые к ним пришли.

Так вот, эксперимент по проверке этой теории мы так и не провели. Потому будем навёрстывать упущенное. А в качестве эксперимента попробуем ILP - Instruction-level parallelism или параллелизм на уровне инструкций. Что это значит - в целом, за счёт знания архитектуры можно выйграть в игру оптимизацию.

К примеру, что мы и собираемся делать, ускорить суммирование 2 массивов. Как помните по примерам прошлого семинара, мы производили вычисления на каждом микроядре отдельно. Теперь вместо этого попробуем отдать одному микроядру сразу несколько ячеек. В чём профит - Количество блоков в очереди уменьшается пропорционально тому, сколько ячеек будет обрабатывать микроядро -> Количество инициализаций и подобных накладных расходов уменьшается -> количество обращений в память не изменяется, то есть выигрыш без особого проигрыша (пока количество блоков больше, чем SM процессоров). Конечно, учитывая, что именно расходы на память у нас пока самые большие, то выигрыш будет довольно мал, но наша цель немного в другом.

Итак, основная задача - проверить, действительно ли случайный доступ к памяти внутри варпа портит всю малину. Для этого сначала возьмём наш код для main из предыдущего семинара, добавим зависимость от ILP (так как каждый поток теперь считает N ячеек, то соответственно, и блоков нам надо в ILP раз меньше)

```c++
int main() {
	int N = 1 << 28;
	size_t size = N * sizeof(float);
	float *x = (float*)malloc(size);
	float *y = (float*)malloc(size);
    
    cudaSetDevice (5);

	float *d_x, *d_y;

	cudaMalloc(&d_x, size);
	cudaMalloc(&d_y, size);


	for (int i = 0; i < N; ++i) {
		x[i] = 1.0f;
		y[i] = 2.0f;
	}


	cudaMemcpy(d_x, x, size, cudaMemcpyHostToDevice);
	cudaMemcpy(d_y, y, size, cudaMemcpyHostToDevice);

	int blockSize = 256;

	int numBlocks = (N + blockSize - 1) / blockSize;

    cudaEvent_t start;
    cudaEvent_t stop;

    // Creating event
    cudaEventCreate(&start);
    cudaEventCreate(&stop);


    cudaEventRecord(start);
	add<<<numBlocks / ILP, blockSize>>>(N, d_x, d_y);
    cudaEventRecord(stop);

    cudaEventSynchronize(stop);
    
    float milliseconds = 0;
    cudaEventElapsedTime(&milliseconds, start, stop);
    std::cout << milliseconds << " elapsed normal" << std::endl;

	cudaFree(d_x);
	cudaFree(d_y);
	free(x);
	free(y);
	return 0;
}
```

В общем, теперь передаём в add numBlocks/ILP блоков. Но этого мало, теперь ещё требуется ядро - одно правильное и одно не правильное. Начнём, пожалуй, со второго, так как проще понять как не правильно))) Идея для порчи малины - требуется доступ к памяти, которая не совпадает с той, которая на текущий момент пришла к потоку. Как попросить - да просто постучать в следующую ячейку вместо оптимальной. Так и сделаем

```c++
__global__
void stupid_add(int n, float* x, float* y) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    int actual_tid = ILP * index;

    for (int i = 0; i < ILP; ++i) {
        int current_tid = actual_tid + i;
        y[current_tid] = 2.0f * x[current_tid] + y[current_tid];
    }
}
```

Как видим - принцип ILP прям по феншую соблюдён - мы заставляем текущий поток обрабатывать не только текущий элемент, но и ровно последующий. Блоков в ILP раз меньше, но при этом каждый поток обрабатывает сразу несколько ячеек массива. Пробуем!.. и получаем на наших видеокартах около 8мс. Стало больше! Уже почти победа. Теперь сделаем нормальный ILP, в котором на warp будут приходить последовательные данные (просто мнимо увеличим в 2 раза блок и будем со сдвигом в обычный блок работать)

```c++
__global__
void add(int n, float* x, float* y) {
    int tid = threadIdx.x + ILP * blockDim.x * blockIdx.x;

    for (int i = 0; i < ILP; ++i) {
        int current_tid = tid + i * blockDim.x;
        
        y[current_tid] = 2.0f * x[current_tid] + y[current_tid];
    }
}
```

Ну то есть притворяемся, что размер блока 512, но и первую и вторую половину блока обрабатывают одни и те-же потоки. И, пробуем! В итоге, к сожалению, получаем всё теже 5мс, но в целом с задачей понимания проблемы доставки данных на варп мы познакомились, что можно считать успехом!
