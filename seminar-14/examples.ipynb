{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чем плох RDD\n",
    "\n",
    "1. Данные рассматриваются как текстовые файлы (или бинарники), разделённые по строкам. Про структуру данных ничего не знаем.\n",
    "  * Каждая задача начинается с парсинга структуры (см. код парсинга с прошлго семинара).\n",
    "  * Иногда нужно работать со столбцами, а не хранить все строки.\n",
    "2. Неудобно. Хочется что-то похожее на SQL и / или pandas. \"Заставляет аналитиков быть разработчиками\" (Д. Лахвич)\n",
    "\n",
    "### Dataframes\n",
    "* Похожи на DF в pandas или SQL.\n",
    "* Работают поверх RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Создаем Spark сессию\n",
    "\n",
    "`.getOrCreate()` - сессия (также как и SparkContext) в рамках приложения существует как Singleton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Spark DF practice').master('yarn').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если работаем с DF, используем SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mipt-client.atp-fivt.org:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f969c79dc50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если нужно RDD - используем SparkContext. Он находится внутри SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mipt-client.atp-fivt.org:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext  # или просто `sc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Читаем данные из источника"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1|24|M|technician|85711\r\n",
      "2|53|F|other|94043\r\n",
      "3|23|M|writer|32067\r\n",
      "4|24|M|technician|43537\r\n",
      "5|33|F|other|15213\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /data/users_csv/u.user | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.1 K  44.2 K  /data/users_csv/u.user\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -du -h /data/users_csv/u.user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 4.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.format(\"csv\")\\\n",
    "          .option(\"sep\", \"|\")\\\n",
    "          .load(\"/data/users_csv/u.user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`options` очень много. Подробнее см. [здесь](https://spark.apache.org/docs/2.4.4/api/java/org/apache/spark/sql/DataFrameReader.html#option-java.lang.String-java.lang.String-) для каждого формата."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему код так долго выполняется? Почему по логике так быть не должно?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df # видим струтуру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно ли там строки? Проверим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----------+-----+\n",
      "|_c0|_c1|_c2|       _c3|  _c4|\n",
      "+---+---+---+----------+-----+\n",
      "|  1| 24|  M|technician|85711|\n",
      "|  2| 53|  F|     other|94043|\n",
      "|  3| 23|  M|    writer|32067|\n",
      "|  4| 24|  M|technician|43537|\n",
      "+---+---+---+----------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть числа (порядковый номер, возраст и ID),  пол (boolean). Надо задать схему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Для каждого поля пишем название, тип и ещё можно указать Nullable\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType(fields=[\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"age\", IntegerType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"occupation\", StringType()),\n",
    "    StructField(\"zip\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 18.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.format(\"csv\")\\\n",
    "          .schema(schema)\\\n",
    "          .option(\"sep\", \"|\")\\\n",
    "          .load(\"/data/users_csv/u.user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, age: int, gender: string, occupation: string, zip: int]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим схему. Без задания схемы, Spark сам пытается вывести типы, но делает это долго и не оч. хорошо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть трансформация `summary`, кот. аналогично pandas, выводит статистику. Это трансформация => для работы нужно вызвать action (show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------+-------------+------------------+\n",
      "|summary|          user_id|              age|gender|   occupation|               zip|\n",
      "+-------+-----------------+-----------------+------+-------------+------------------+\n",
      "|  count|              925|              925|   925|          925|               925|\n",
      "|   mean|470.2908108108108|34.06054054054054|  null|         null| 50868.78810810811|\n",
      "| stddev|272.1030147185632|12.25807489536592|  null|         null|30891.373254138176|\n",
      "|    min|                1|                7|     F|administrator|                 0|\n",
      "|    25%|              236|               25|  null|         null|             21227|\n",
      "|    50%|              469|               31|  null|         null|             53711|\n",
      "|    75%|              705|               43|  null|         null|             78741|\n",
      "|    max|              943|               73|     M|       writer|             99835|\n",
      "+-------+-----------------+-----------------+------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование из RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RDD разделители внутри строк не видит, нужно парсить\n",
    "rdd = sc.textFile(\"/data/users_csv/u.user\").map(lambda x: x.split(\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '24', 'M', 'technician', '85711'],\n",
       " ['2', '53', 'F', 'other', '94043'],\n",
       " ['3', '23', 'M', 'writer', '32067'],\n",
       " ['4', '24', 'M', 'technician', '43537'],\n",
       " ['5', '33', 'F', 'other', '15213']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 164 ms, sys: 20 ms, total: 184 ms\n",
      "Wall time: 385 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Конвертируем в DF\n",
    "df = spark.createDataFrame(rdd, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o98.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 8, mipt-node06.atp-fivt.org, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/serializers.py\", line 393, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/session.py\", line 730, in prepare\n    verify_func(obj)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1370, in verify_struct\n    verifier(v)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1315, in verify_integer\n    verify_acceptable_types(obj)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1278, in verify_acceptable_types\n    % (dataType, obj, type(obj))))\nTypeError: field user_id: IntegerType can not accept object '1' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/serializers.py\", line 393, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/session.py\", line 730, in prepare\n    verify_func(obj)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1370, in verify_struct\n    verifier(v)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1315, in verify_integer\n    verify_acceptable_types(obj)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1278, in verify_acceptable_types\n    % (dataType, obj, type(obj))))\nTypeError: field user_id: IntegerType can not accept object '1' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fbfcfd3ff210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o98.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 8, mipt-node06.atp-fivt.org, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/serializers.py\", line 393, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/session.py\", line 730, in prepare\n    verify_func(obj)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1370, in verify_struct\n    verifier(v)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1315, in verify_integer\n    verify_acceptable_types(obj)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1278, in verify_acceptable_types\n    % (dataType, obj, type(obj))))\nTypeError: field user_id: IntegerType can not accept object '1' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/serializers.py\", line 393, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/session.py\", line 730, in prepare\n    verify_func(obj)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1370, in verify_struct\n    verifier(v)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1315, in verify_integer\n    verify_acceptable_types(obj)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/types.py\", line 1278, in verify_acceptable_types\n    % (dataType, obj, type(obj))))\nTypeError: field user_id: IntegerType can not accept object '1' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Отключаем проверку схемы\n",
    "df = spark.createDataFrame(rdd, schema=schema, verifySchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+----------+----+\n",
      "|user_id| age|gender|occupation| zip|\n",
      "+-------+----+------+----------+----+\n",
      "|   null|null|     M|technician|null|\n",
      "|   null|null|     F|     other|null|\n",
      "|   null|null|     M|    writer|null|\n",
      "|   null|null|     M|technician|null|\n",
      "|   null|null|     F|     other|null|\n",
      "+-------+----+------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибок нет... и данных тоже. Что делать? Допиливать RDD :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+-----+\n",
      "|user_id|age|gender|occupation|  zip|\n",
      "+-------+---+------+----------+-----+\n",
      "|      1| 24|     M|technician|85711|\n",
      "|      2| 53|     F|     other|94043|\n",
      "|      3| 23|     M|    writer|32067|\n",
      "|      4| 24|     M|technician|43537|\n",
      "|      5| 33|     F|     other|15213|\n",
      "+-------+---+------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = rdd.map(lambda x: (int(x[0]), int(x[1]), x[2], x[3], int(x[4])))\n",
    "df = spark.createDataFrame(rdd, schema=schema)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.49.147.163\t20140101014611\thttp://news.rambler.ru/3105700\t378\t431\tSafari/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729;)\r",
      "\r\n",
      "197.72.248.141\t20140101020306\thttp://news.mail.ru/6344933\t1412\t203\tSafari/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729;\r",
      "\r\n",
      "33.49.147.163\t20140101023103\thttp://lenta.ru/4303000\t1189\t451\tChrome/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0)\r",
      "\r\n",
      "75.208.40.166\t20140101032909\thttp://newsru.com/3330815\t60\t306\tSafari/5.0 (Windows; U; MSIE 9.0; Windows NT 8.1; Trident/5.0; .NET4.0E; en-AU)\r",
      "\r\n",
      "197.72.248.141\t20140101033626\thttp://newsru.com/1588977\t736\t307\tChrome/5.0 (compatible; MSIE 9.0; Windows NT 8.0; WOW64; Trident/5.0; .NET CLR 2.7.40781; .NET4.0E; en-SG)\r",
      "\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /data/user_logs/logsM2.txt | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_schema = StructType(fields=[\n",
    "    StructField(\"ip\", StringType()),\n",
    "    StructField(\"timestamp\", LongType()),\n",
    "    StructField(\"url\", StringType()),\n",
    "    StructField(\"size\", IntegerType()),\n",
    "    StructField(\"code\", IntegerType()),\n",
    "    StructField(\"ua\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_df = spark.read.csv(\"/data/user_logs/logsM2.txt\", sep=\"\\t\", schema=log_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------------------------------------\n",
      " ip        | 33.49.147.163                                                                                                 \n",
      " timestamp | 20140101014611                                                                                                \n",
      " url       | http://news.rambler.ru/3105700                                                                                \n",
      " size      | 378                                                                                                           \n",
      " code      | 431                                                                                                           \n",
      " ua        | Safari/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729;)               \n",
      "-RECORD 1------------------------------------------------------------------------------------------------------------------\n",
      " ip        | 197.72.248.141                                                                                                \n",
      " timestamp | 20140101020306                                                                                                \n",
      " url       | http://news.mail.ru/6344933                                                                                   \n",
      " size      | 1412                                                                                                          \n",
      " code      | 203                                                                                                           \n",
      " ua        | Safari/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; \n",
      "-RECORD 2------------------------------------------------------------------------------------------------------------------\n",
      " ip        | 33.49.147.163                                                                                                 \n",
      " timestamp | 20140101023103                                                                                                \n",
      " url       | http://lenta.ru/4303000                                                                                       \n",
      " size      | 1189                                                                                                          \n",
      " code      | 451                                                                                                           \n",
      " ua        | Chrome/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0)                                    \n",
      "-RECORD 3------------------------------------------------------------------------------------------------------------------\n",
      " ip        | 75.208.40.166                                                                                                 \n",
      " timestamp | 20140101032909                                                                                                \n",
      " url       | http://newsru.com/3330815                                                                                     \n",
      " size      | 60                                                                                                            \n",
      " code      | 306                                                                                                           \n",
      " ua        | Safari/5.0 (Windows; U; MSIE 9.0; Windows NT 8.1; Trident/5.0; .NET4.0E; en-AU)                               \n",
      "-RECORD 4------------------------------------------------------------------------------------------------------------------\n",
      " ip        | 197.72.248.141                                                                                                \n",
      " timestamp | 20140101033626                                                                                                \n",
      " url       | http://newsru.com/1588977                                                                                     \n",
      " size      | 736                                                                                                           \n",
      " code      | 307                                                                                                           \n",
      " ua        | Chrome/5.0 (compatible; MSIE 9.0; Windows NT 8.0; WOW64; Trident/5.0; .NET CLR 2.7.40781; .NET4.0E; en-SG)    \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.show(5, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df.rdd.getNumPartitions() # файл 1, значит и партиция одна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df = log_df.repartition(4) # Увеличим число партиций\n",
    "log_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простые действия над DF\n",
    "Вспоминаем реляционку:\n",
    " * Проекция (`SELECT`) - подмножество столбцов\n",
    " * Фильтр (`WHERE`, `HAVING`) - подмножество строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ip', 'timestamp', 'url', 'size', 'code', 'ua']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df.schema.fieldNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проекция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------------+\n",
      "|             ip|     timestamp|                 url|\n",
      "+---------------+--------------+--------------------+\n",
      "| 197.72.248.141|20140313153714|http://lenta.ru/9...|\n",
      "|  75.208.40.166|20140210083843|http://news.rambl...|\n",
      "| 222.131.187.37|20140425170839|http://lenta.ru/2...|\n",
      "|   49.203.96.67|20140225042119|http://news.yande...|\n",
      "|135.124.143.193|20140313110843|http://news.mail....|\n",
      "+---------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.select([\"ip\", \"timestamp\", \"url\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ip: string, timestamp: bigint, url: string]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df.select(*log_df.schema.fieldNames()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ip: string, timestamp: bigint, url: string]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df.select(\"ip\", \"timestamp\", \"url\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это всё трансформации. На выходе получаем DF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------+----+----+--------------------+\n",
      "|            ip|     timestamp|                 url|size|code|                  ua|\n",
      "+--------------+--------------+--------------------+----+----+--------------------+\n",
      "| 75.208.40.166|20140326084231|http://news.yande...| 955| 200|Opera/5.0 (Window...|\n",
      "|197.72.248.141|20140404122749|http://newsru.com...| 884| 200|Opera/5.0 (compat...|\n",
      "| 75.208.40.166|20140216075757|http://news.rambl...|1337| 200|Firefox/5.0 (comp...|\n",
      "|168.255.93.197|20140213033139|http://news.yande...|1004| 200|Chrome/5.0 (Windo...|\n",
      "|197.72.248.141|20140403130026|http://news.yande...| 856| 200|Opera/5.0 (compat...|\n",
      "+--------------+--------------+--------------------+----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.where(\"code == 200\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------+----+----+--------------------+\n",
      "|            ip|     timestamp|                 url|size|code|                  ua|\n",
      "+--------------+--------------+--------------------+----+----+--------------------+\n",
      "| 75.208.40.166|20140326084231|http://news.yande...| 955| 200|Opera/5.0 (Window...|\n",
      "|197.72.248.141|20140404122749|http://newsru.com...| 884| 200|Opera/5.0 (compat...|\n",
      "| 75.208.40.166|20140216075757|http://news.rambl...|1337| 200|Firefox/5.0 (comp...|\n",
      "|168.255.93.197|20140213033139|http://news.yande...|1004| 200|Chrome/5.0 (Windo...|\n",
      "|197.72.248.141|20140403130026|http://news.yande...| 856| 200|Opera/5.0 (compat...|\n",
      "+--------------+--------------+--------------------+----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.filter(log_df.code == 200).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+--------------------+----+----+--------------------+\n",
      "|           ip|     timestamp|                 url|size|code|                  ua|\n",
      "+-------------+--------------+--------------------+----+----+--------------------+\n",
      "|33.49.147.163|20140317090245|http://news.rambl...| 166| 200|Safari/5.0 (Windo...|\n",
      "|75.208.40.166|20140310132240|http://news.rambl...|1107| 200|Opera/5.0 (compat...|\n",
      "|33.49.147.163|20140330043133|http://news.rambl...| 945| 200|Chrome/5.0 (compa...|\n",
      "|33.49.147.163|20140329134621|http://news.rambl...|1593| 200|Firefox/5.0 (comp...|\n",
      "|33.49.147.163|20140106045941|http://news.rambl...| 671| 200|Firefox/5.0 (comp...|\n",
      "+-------------+--------------+--------------------+----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.filter(\"code == 200 AND url LIKE '%rambler%'\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+--------------------+----+----+--------------------+\n",
      "|           ip|     timestamp|                 url|size|code|                  ua|\n",
      "+-------------+--------------+--------------------+----+----+--------------------+\n",
      "|33.49.147.163|20140317090245|http://news.rambl...| 166| 200|Safari/5.0 (Windo...|\n",
      "|75.208.40.166|20140310132240|http://news.rambl...|1107| 200|Opera/5.0 (compat...|\n",
      "|33.49.147.163|20140330043133|http://news.rambl...| 945| 200|Chrome/5.0 (compa...|\n",
      "|33.49.147.163|20140329134621|http://news.rambl...|1593| 200|Firefox/5.0 (comp...|\n",
      "|33.49.147.163|20140106045941|http://news.rambl...| 671| 200|Firefox/5.0 (comp...|\n",
      "+-------------+--------------+--------------------+----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.filter(log_df.code.isin([200, 399]) & log_df.url.like(\"%rambler%\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|             ip|response_code|\n",
      "+---------------+-------------+\n",
      "|   25.62.10.220|          306|\n",
      "|   49.105.15.79|          101|\n",
      "|   49.105.15.79|          101|\n",
      "|135.124.143.193|          303|\n",
      "|135.124.143.193|          303|\n",
      "+---------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.select(log_df.ip, log_df.code.alias(\"response_code\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|             ip|response_code|\n",
      "+---------------+-------------+\n",
      "|   25.62.10.220|          306|\n",
      "|   49.105.15.79|          101|\n",
      "|   49.105.15.79|          101|\n",
      "|135.124.143.193|          303|\n",
      "|135.124.143.193|          303|\n",
      "+---------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "log_df.select(\"ip\", f.col(\"code\").alias(\"response_code\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+\n",
      "|             ip|code|\n",
      "+---------------+----+\n",
      "|   25.62.10.220| 306|\n",
      "|   49.105.15.79| 101|\n",
      "|   49.105.15.79| 101|\n",
      "|135.124.143.193| 303|\n",
      "|135.124.143.193| 303|\n",
      "+---------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df[log_df.ip, \"code\"].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+\n",
      "|            ip|code|\n",
      "+--------------+----+\n",
      "|  3.183.113.77| 404|\n",
      "|110.91.102.196| 404|\n",
      "|168.255.93.197| 404|\n",
      "|197.72.248.141| 404|\n",
      "|197.72.248.141| 404|\n",
      "+--------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df[(log_df.code == 404) & (log_df.url.like(\"%rambler%\"))][log_df.ip, \"code\"].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_str = \"\"\"\n",
    "SELECT ip, code FROM log_df\n",
    "WHERE code == 404 AND url LIKE \"%rambler%\" \n",
    "\"\"\"\n",
    "log_df.registerTempTable(\"log_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+\n",
      "|            ip|code|\n",
      "+--------------+----+\n",
      "|  3.183.113.77| 404|\n",
      "|110.91.102.196| 404|\n",
      "|168.255.93.197| 404|\n",
      "|197.72.248.141| 404|\n",
      "|197.72.248.141| 404|\n",
      "+--------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query_str).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                  ua|length|\n",
      "+--------------------+------+\n",
      "|Safari/5.0 (compa...|    95|\n",
      "|Safari/5.0 (compa...|    95|\n",
      "|Safari/5.0 (compa...|    95|\n",
      "|Safari/5.0 (compa...|    95|\n",
      "|Safari/5.0 (compa...|    95|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.select(\"ua\", f.length(\"ua\").alias(\"length\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------\n",
      " my  | http://newsru.com/4608402?key=value&key2=val2      \n",
      "-RECORD 1-------------------------------------------------\n",
      " my  | http://news.yandex.ru/4761980?key=value&key2=val2  \n",
      "-RECORD 2-------------------------------------------------\n",
      " my  | http://lenta.ru/2662415?key=value&key2=val2        \n",
      "-RECORD 3-------------------------------------------------\n",
      " my  | http://news.mail.ru/5550951?key=value&key2=val2    \n",
      "-RECORD 4-------------------------------------------------\n",
      " my  | http://news.rambler.ru/2396720?key=value&key2=val2 \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.select(f.concat(\"url\", f.lit(\"?key=value&key2=val2\")).alias('my')).show(5, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------------------------------------\n",
      " ua           | Safari/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; chromeframe/12.0.742.112)            \n",
      " split(ua,  ) | [Safari/5.0, (compatible;, MSIE, 9.0;, Windows, NT, 6.1;, WOW64;, Trident/5.0;, chromeframe/12.0.742.112)] \n",
      "-RECORD 1------------------------------------------------------------------------------------------------------------------\n",
      " ua           | Safari/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; chromeframe/12.0.742.112)            \n",
      " split(ua,  ) | [Safari/5.0, (compatible;, MSIE, 9.0;, Windows, NT, 6.1;, WOW64;, Trident/5.0;, chromeframe/12.0.742.112)] \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.select('ua', f.split('ua', ' ')).show(2, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "| parsed[0]|   parsed[8]|\n",
      "+----------+------------+\n",
      "|Safari/5.0|Trident/5.0;|\n",
      "|Safari/5.0|Trident/5.0;|\n",
      "|Safari/5.0|Trident/5.0;|\n",
      "|Safari/5.0|Trident/5.0;|\n",
      "|Safari/5.0|Trident/5.0;|\n",
      "+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.select('ua', f.split('ua', ' ').alias('parsed')).select(f.col('parsed')[0], f.col('parsed')[8]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|     parsed_exploded|count|\n",
      "+--------------------+-----+\n",
      "|          3.0.30729;|  962|\n",
      "|          3.5.30729;|  962|\n",
      "|        Trident/5.0)|  971|\n",
      "|chromeframe/12.0....|  983|\n",
      "|                 en)| 1004|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.select('ua', f.split('ua', ' ').alias('parsed')).select(f.explode('parsed').alias('parsed_exploded'))\\\n",
    "    .groupby('parsed_exploded').count().orderBy(\"count\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.105.15.79\tKomi\r\n",
      "110.91.102.196\tChelyabinsk Oblast\r\n",
      "56.167.169.126\tSaint Petersburg\r\n",
      "75.208.40.166\tUlyanovsk Oblast\r\n",
      "168.255.93.197\tIrkutsk Oblast\r\n",
      "75.208.40.166\tArkhangelsk Oblast\r\n",
      "110.91.102.196\tZabaykalsky Krai\r\n",
      "75.208.40.166\tTyumen Oblast\r\n",
      "56.167.169.126\tTomsk Oblast\r\n",
      "75.208.40.166\tSakha\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /data/user_logs/ip_data_M/ipDataM.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239.3 K  478.6 K  /data/user_logs/ip_data_M/ipDataM.txt\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -du -h /data/user_logs/ip_data_M/ipDataM.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ip_schema = StructType(fields=[\n",
    "    StructField('ip', StringType()),\n",
    "    StructField('region', StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ips = spark.read.csv(\"/data/user_logs/ip_data_M/ipDataM.txt\", sep=\"\\t\", schema=ip_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|            ip|            region|\n",
      "+--------------+------------------+\n",
      "|  49.105.15.79|              Komi|\n",
      "|110.91.102.196|Chelyabinsk Oblast|\n",
      "|56.167.169.126|  Saint Petersburg|\n",
      "| 75.208.40.166|  Ulyanovsk Oblast|\n",
      "|168.255.93.197|    Irkutsk Oblast|\n",
      "+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ips.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SET spark.sql.autoBroadcastJoinThreshold = 100\")\n",
    "logs_with_reg = log_df.join(ips, on='ip', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [ip#505, timestamp#506L, url#507, size#508, code#509, ua#510, region#801]\n",
      "+- *(5) SortMergeJoin [ip#505], [ip#800], Inner\n",
      "   :- *(2) Sort [ip#505 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(ip#505, 200)\n",
      "   :     +- Exchange RoundRobinPartitioning(4)\n",
      "   :        +- *(1) Project [ip#505, timestamp#506L, url#507, size#508, code#509, ua#510]\n",
      "   :           +- *(1) Filter isnotnull(ip#505)\n",
      "   :              +- *(1) FileScan csv [ip#505,timestamp#506L,url#507,size#508,code#509,ua#510] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://mipt-master.atp-fivt.org:8020/data/user_logs/logsM2.txt], PartitionFilters: [], PushedFilters: [IsNotNull(ip)], ReadSchema: struct<ip:string,timestamp:bigint,url:string,size:int,code:int,ua:string>\n",
      "   +- *(4) Sort [ip#800 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(ip#800, 200)\n",
      "         +- *(3) Project [ip#800, region#801]\n",
      "            +- *(3) Filter isnotnull(ip#800)\n",
      "               +- *(3) FileScan csv [ip#800,region#801] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://mipt-master.atp-fivt.org:8020/data/user_logs/ip_data_M/ipDataM.txt], PartitionFilters: [], PushedFilters: [IsNotNull(ip)], ReadSchema: struct<ip:string,region:string>\n"
     ]
    }
   ],
   "source": [
    "logs_with_reg.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+--------------------+----+----+--------------------+-------------------+\n",
      "|          ip|     timestamp|                 url|size|code|                  ua|             region|\n",
      "+------------+--------------+--------------------+----+----+--------------------+-------------------+\n",
      "|3.183.113.77|20140126111936|http://newsru.com...|1485| 412|Firefox/5.0 compa...|           Chukotka|\n",
      "|3.183.113.77|20140126111936|http://newsru.com...|1485| 412|Firefox/5.0 compa...|     Ivanovo Oblast|\n",
      "|3.183.113.77|20140126111936|http://newsru.com...|1485| 412|Firefox/5.0 compa...|          Tatarstan|\n",
      "|3.183.113.77|20140126111936|http://newsru.com...|1485| 412|Firefox/5.0 compa...|Karachay–Cherkessia|\n",
      "|3.183.113.77|20140126111936|http://newsru.com...|1485| 412|Firefox/5.0 compa...|   Yaroslavl Oblast|\n",
      "+------------+--------------+--------------------+----+----+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_with_reg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_with_reg.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Про `coalesce()` и `repartition()`: [Link](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.coalesce.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [ip#505, timestamp#506L, url#507, size#508, code#509, ua#510, region#801]\n",
      "+- *(3) BroadcastHashJoin [ip#505], [ip#800], Inner, BuildRight\n",
      "   :- Exchange RoundRobinPartitioning(4)\n",
      "   :  +- *(1) Project [ip#505, timestamp#506L, url#507, size#508, code#509, ua#510]\n",
      "   :     +- *(1) Filter isnotnull(ip#505)\n",
      "   :        +- *(1) FileScan csv [ip#505,timestamp#506L,url#507,size#508,code#509,ua#510] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://mipt-master.atp-fivt.org:8020/data/user_logs/logsM2.txt], PartitionFilters: [], PushedFilters: [IsNotNull(ip)], ReadSchema: struct<ip:string,timestamp:bigint,url:string,size:int,code:int,ua:string>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "      +- *(2) Project [ip#800, region#801]\n",
      "         +- *(2) Filter isnotnull(ip#800)\n",
      "            +- *(2) FileScan csv [ip#800,region#801] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://mipt-master.atp-fivt.org:8020/data/user_logs/ip_data_M/ipDataM.txt], PartitionFilters: [], PushedFilters: [IsNotNull(ip)], ReadSchema: struct<ip:string,region:string>\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "spark.sql(\"SET spark.sql.autoBroadcastJoinThreshold = 10\")\n",
    "logs_with_reg = log_df.join(f.broadcast(ips), on='ip', how='inner')\n",
    "logs_with_reg.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_with_reg.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+--------------------+----+----+--------------------+--------------------+\n",
      "|          ip|     timestamp|                 url|size|code|                  ua|              region|\n",
      "+------------+--------------+--------------------+----+----+--------------------+--------------------+\n",
      "|49.105.15.79|20140127041332|http://lenta.ru/5...| 184| 509|Chrome/5.0 compat...|North Ossetia–Alania|\n",
      "|49.105.15.79|20140127041332|http://lenta.ru/5...| 184| 509|Chrome/5.0 compat...|            Kalmykia|\n",
      "|49.105.15.79|20140127041332|http://lenta.ru/5...| 184| 509|Chrome/5.0 compat...|      Bryansk Oblast|\n",
      "|49.105.15.79|20140127041332|http://lenta.ru/5...| 184| 509|Chrome/5.0 compat...|        Kursk Oblast|\n",
      "|49.105.15.79|20140127041332|http://lenta.ru/5...| 184| 509|Chrome/5.0 compat...|              Udmurt|\n",
      "+------------+--------------+--------------------+----+----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_with_reg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+\n",
      "|           region|count(ip)|\n",
      "+-----------------+---------+\n",
      "|    Kaluga Oblast|   111297|\n",
      "|    Ryazan Oblast|    80716|\n",
      "|  Smolensk Oblast|    99735|\n",
      "|Sverdlovsk Oblast|    87004|\n",
      "|          Mari El|    98414|\n",
      "+-----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_with_reg.groupby('region').agg(f.count('ip')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = logs_with_reg.groupby('region').count().distinct().coalesce(2).withColumnRenamed('count', 'ip_count')\n",
    "result.rdd.getNumPartitions()\n",
    "#result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.write.csv(\"region_counts.tsv\", sep='\\t', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   2 mtsion mtsion          0 2021-11-27 19:45 region_counts.tsv/_SUCCESS\r\n",
      "-rw-r--r--   2 mtsion mtsion        809 2021-11-27 19:45 region_counts.tsv/part-00000-711fe1f7-2487-491f-9403-5bf5da013ead-c000.csv\r\n",
      "-rw-r--r--   2 mtsion mtsion        841 2021-11-27 19:45 region_counts.tsv/part-00001-711fe1f7-2487-491f-9403-5bf5da013ead-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls region_counts.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: `region_counts.tsv/part-00001-6da128c0-e084-4ae0-9c37-e603987abf44-c000.csv': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# ! hdfs dfs -ls region_counts.tsv\n",
    "! hdfs dfs -cat region_counts.tsv/part-00001-6da128c0-e084-4ae0-9c37-e603987abf44-c000.csv | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "logs_with_reg.select(\"ip\", \"timestamp\", f.count(\"*\").over(Window.partitionBy('ip')).alias(\"cnt\")).orderBy(\"cnt\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи \n",
    "\n",
    "В hdfs в папке `/data/access_logs/big_log` лежит лог в формате\n",
    "\n",
    "* IP-адрес пользователя (`195.206.123.39`),\n",
    "* Далее идут два неиспользуемых в нашем случае поля (`-` и `-`),\n",
    "* Время запроса (`[24/Sep/2015:12:32:53 +0400]`),\n",
    "* Строка запроса (`\"GET /id18222 HTTP/1.1\"`),\n",
    "* HTTP-код ответа (`200`),\n",
    "* Размер ответа (`10703`),\n",
    "* Реферер (источник перехода; `\"http://bing.com/\"`),\n",
    "* Идентификационная строка браузера (User-Agent; `\"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"`).\n",
    "\n",
    "Созданы несколько семплов данных разного размера:\n",
    "```\n",
    "3.4 G    10.2 G   /data/access_logs/big_log\n",
    "17.6 M   52.7 M   /data/access_logs/big_log_10000\n",
    "175.4 M  526.2 M  /data/access_logs/big_log_100000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from datetime import datetime as dt\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "log_format = re.compile(\n",
    "    r\"(?P<host>[\\d\\.]+)\\s\"\n",
    "    r\"(?P<identity>\\S*)\\s\"\n",
    "    r\"(?P<user>\\S*)\\s\"\n",
    "    r\"\\[(?P<time>.*?)\\]\\s\"\n",
    "    r'\"(?P<request>.*?)\"\\s'\n",
    "    r\"(?P<status>\\d+)\\s\"\n",
    "    r\"(?P<bytes>\\S*)\\s\"\n",
    "    r'\"(?P<referer>.*?)\"\\s'\n",
    "    r'\"(?P<user_agent>.*?)\"\\s*'\n",
    ")\n",
    "\n",
    "\n",
    "def parseLine(line):\n",
    "    match = log_format.match(line)\n",
    "    if not match:\n",
    "        return (\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\")\n",
    "\n",
    "    request = match.group('request').split()\n",
    "    return (match.group('host'), match.group('time').split()[0],\n",
    "        request[0], request[1], match.group('status'), int(match.group('bytes')),\n",
    "        match.group('referer'), match.group('user_agent'),\n",
    "        dt.strptime(match.group('time').split()[0], '%d/%b/%Y:%H:%M:%S').hour)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark_session = SparkSession.builder.master(\"yarn\").appName(\"501 df\").config(\"spark.ui.port\", \"18089\").getOrCreate()\n",
    "    lines = spark_session.sparkContext.textFile(\"/data/access_logs/big_log_10000\")\n",
    "    parts = lines.map(parseLine)\n",
    "    rows = parts.map(lambda p: Row(ip=p[0],\n",
    "                                   timestamp=p[1],\n",
    "                                   request_type=p[2],\n",
    "                                   request_url=p[3],\n",
    "                                   status=p[4],\n",
    "                                   bytes=p[5],\n",
    "                                   referer=p[6],\n",
    "                                   user_agent=p[7],\n",
    "                                   hour=p[8]))\n",
    "    access_log_df = spark_session.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------------+-------+------------+-----------+------+--------------------+--------------------+\n",
      "|bytes|hour|             ip|referer|request_type|request_url|status|           timestamp|          user_agent|\n",
      "+-----+----+---------------+-------+------------+-----------+------+--------------------+--------------------+\n",
      "|27513|   0|109.105.128.100|      -|         GET|   /id45574|   200|10/Dec/2015:00:00:00|Mozilla/5.0 (Wind...|\n",
      "|11914|   0| 217.146.45.122|      -|         GET|   /id40851|   200|10/Dec/2015:00:00:00|Mozilla/5.0 (X11;...|\n",
      "|32457|   0|   17.72.78.198|      -|         GET|   /id58931|   200|10/Dec/2015:00:00:00|Mozilla/5.0; TOB ...|\n",
      "|26190|   0|  46.245.183.68|      -|         GET|   /id19513|   200|10/Dec/2015:00:00:00|Mozilla/5.0 (Wind...|\n",
      "|14115|   0| 91.197.164.156|      -|         GET|   /id39028|   200|10/Dec/2015:00:00:01|Mozilla/5.0 (X11;...|\n",
      "+-----+----+---------------+-------+------------+-----------+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "access_log_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1.\n",
    "> Напишите программу, выводящую на экран TOP5 ip адресов, в которых содержится хотя бы одна цифра 4, с наибольшим количеством посещений.\n",
    "Каждая строка результата должна содержать IP адрес и число посещений, разделенные табуляцией, строки должны быть упорядочены по числу посещений по убыванию, например:\n",
    "\n",
    "```\n",
    "195.206.123.39<TAB>40\n",
    "196.206.123.40<TAB>39\n",
    "191.206.123.41<TAB>38\n",
    "175.206.123.42<TAB>37\n",
    "195.236.123.43<TAB>36\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 5.\n",
    "\n",
    ">  Напишите программу, выводящую на экран суммарное распределение количества посетителей по часам (для каждого часа в сутках вывести количество посетителей, пришедших в этот час). Id посетителя = ip + user_agent.\n",
    "Результат должен содержать час в сутках и число посетителей, разделенные табом и упорядоченные по часам. Например:\n",
    "```\n",
    "0<tab>10\n",
    "1<tab>10\n",
    "2<tab>10\n",
    "…..\n",
    "21<tab>30\n",
    "22<tab>20\n",
    "23<tab>10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# .drop_duplicates([\"ip\", \"user_agent\", \"hour\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
