{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Недостатки MapReduce\n",
    "1) **Минимум 2 итерации записи на HDD** за job'у => не работает в реальном времени. Хотелось бы хранить сплиты в RAM.\n",
    "\n",
    "2) **Парадигма коротко живущих контейнеров.** Вспоминаем лекцию по YARN (приложения на сервис, приложения на задачу...). \n",
    "    * При запуске mapreduce-таски (например, маппера) YARN запустит контейнер\n",
    "    * контейнер умрёт его когда таска завершится. \n",
    "    * При аварии контейнер перезапустится на другой машине. \n",
    "Это удобно но старт-стоп контейнеров даёт Overhead если MapReduce-задач много.\n",
    "\n",
    "3) Нужно писать **очень много кода** (вспоминаем задачу на Join с позапрошлого семинара).\n",
    "\n",
    "4) По сути 1 источник данных - диск (HDFS, локальная ФС клиента... но всё равно диск). Хотелось бы уметь читать / писать в другие источники (базы данных, облачные хранилища).\n",
    "\n",
    "**Итог:** с MapReduce **можно** работать с BigData, но нельзя работать быстро."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* **200Х годы**: нам нужна отказоустойчивая система. RAM на серверах мало. Будем сохранять промежуточные данные **на диск**.\n",
    "* **201Х годы**: \n",
    "    - Память становится дешевле и больше. \n",
    "    - Запросы от бизнеса на максимально быструю обработку (real-time).\n",
    " \n",
    "Диск использовать нецелесообразно - возвращаемся к RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Составляющие Spark-экосистемы\n",
    "\n",
    "Spark Написан на Scala, имеет Scala, Java, Python API.\n",
    "\n",
    "1. Spark Core - разбор кода, распределённое выполнение, поддержка отказоустойчивости.\n",
    "2. Аналог \"стандартной библиотеки\":\n",
    "   * Spark SQL - высокоуровненвая обработка с помощью pandas-подобного синтаксиса или SQL.\n",
    "   * Spark Streaming, Spark Structured Streaming - обработка (обновление результатов) данных в real-time\n",
    "   * MLLib - инструментарий для ML. Помимо Spark использует сторонние библиотеки (например Breeze, написанный на Fortran).\n",
    "3. Планировщики:\n",
    "   * Standalone - легковесный Spark на 1 машине. Использует встроенный планировщик\n",
    "   * Может использовать другие планировщики (например YARN, Mesos, Kubernetes).\n",
    "\n",
    "Подробнее **[здесь](https://www.oreilly.com/library/view/learning-spark/9781449359034/ch01.html)**.\n",
    "\n",
    "#### Источники данных\n",
    "![Image](images/datasources.png)\n",
    "\n",
    "В теории можем читать-писать в большое кол-во источников и приёмников данных.\n",
    "\n",
    "На практике:\n",
    "* Есть проблемы при взаимодействии с Hive (подробнее будет на лекции),\n",
    "* И при подключении к Cassandra.\n",
    "* Хорошо взаимодействует с Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Архитектура Spark-приложения\n",
    "![Image](images/cluster-overview.png)\n",
    "(https://spark.apache.org/docs/latest/cluster-overview.html)\n",
    "\n",
    "1. Driver program - управляющая программа.\n",
    "2. SparkContext - это основной объект, с помощью которого мы взаимодействуем со Spark.\n",
    "3. Cluster manager - планировщик (любой, см. выше).\n",
    "4. Executor - по сути JVM на нодах.\n",
    "\n",
    "В 1-м приближении работает также как и Hadoop. Единственное, контейнеры **долго живущие**. Контейнеры поднимаются 1 раз и умирают когда заканчивается SparkContext. Это позволяет хранить данные **в памяти JVM**. Быстрее RAM только кеши CPU, но это сложно реализуется (ассемблер)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Возможности работы со Spark\n",
    "##### Интерактивный shell\n",
    "\n",
    "1. `spark2-shell` - запускает Scala-оболочку.\n",
    "2. `pyspark2` - python оболочку.\n",
    "\n",
    "В этих оболочках уже имеется готовый SparkContext (переменная `sc`).\n",
    "\n",
    "##### Запуск файла на исполнение\n",
    "`spark2-submit [params] <file>` - можем запускать как jar-файлы, так и коды на Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Запуск Spark в Jupyter-ноутбуке:\n",
    "\n",
    "```bash\n",
    "PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_PYTHON=/usr/bin/python3 PYSPARK_DRIVER_PYTHON_OPTS='notebook --ip=\"*\" --port=<PORT> --NotebookApp.token=\"<TOKEN>\" --no-browser' pyspark2 --master=yarn --num-executors=<N>\n",
    "```\n",
    " - **PORT** - порт, на котором откроется ноутбук.\n",
    " - **TOKEN** - токен, который нужно будет ввести для входа в Jupyter (любая строка). Не оставляйте токен пустым т.к. в этом случае к вашему ноутбуку смогут подключаться другие пользователи. `--NotebookApp.token=\"<TOKEN>\"` можно не писать, тогда он сгенерится сам, а посмотреть его можно будет с помощью команды `jupyter notebook list`.\n",
    " - **N** - кол-во executors (YARN containers), выделенных на приложение. \n",
    " \n",
    "Подробнее в [Userguide](https://docs.google.com/document/d/1dmb8o3M2ZCsjPq3rJQqd-jNLQhiBXWbWZcTn9aYUAp8/edit).\n",
    " \n",
    "#### Режимы запуска Spark\n",
    "1. **local**. И драйвер, и worker стартуют на 1 машине. Можно указывать число ядер, выделенных на задачу. Например, `local[3]`. Указывать меньше 2 не рекомендуется т.к. всегда запускает 2 процесса: driver, worker.\n",
    "2. **yarn**. Распределённый режим. Здесь можно дополнительно указать `--deploy-mode`. \n",
    "   * `cluster`. Драйвер на мастере либо на ноде. Рекомендуется для прода.\n",
    "   * `client`. Драйвер на клиенте. Проще отлаживаться. Проще работать в интерактивном режиме (сейчас мы работаем в режиме `client`). Но грузит клиент. \n",
    " \n",
    "В аргументах PySpark можно указывать и многе другое, подробнее [здесь](http://spark.apache.org/docs/latest/configuration.html#application-properties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Можем изменить конфигурацию SparkContext, правда его придётся перезапустить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "conf = sc.getConf().setAppName(\"the {}\\'s spark app\".format(getpass.getuser())).set(\"spark.python.profile\",\"true\")\n",
    "sc.stop()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/griboedov\").map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sc.show_profiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Resilient Distributed Dataset и ленивые вычисления\n",
    "\n",
    "RDD - набор данных, распределённый по партициям (аналог сплитов в Hadoop). Основной примитив работы в Spark. \n",
    "\n",
    "##### Свойства\n",
    "* Неизменяемый. Можем получить либо новый RDD, либо plain object\n",
    "* Итерируемый. Можем делать обход RDD\n",
    "* Восстанавливаемый. Каждая партиция помнит как она была получена (часть графа вычислений) и при утере может быть восстановлена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Создать RDD можно:\n",
    "* прочитав данные из источника\n",
    "* получить новый RDD из существующего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /data/griboedov/gore_ot_uma-1.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/griboedov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Идём в [SparkHistory UI](http://localhost:18089/) (для этого нужно пробросить порт 18089). Далее переходим в incompleted applications (приложение не завершилось т.к. SparkContext жив) и видим, что в списке Job пусто.\n",
    "\n",
    "Несмотря на это, сам RDD есть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rdd.map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Посчитаем кол-во объектов в RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Снова проверяем UI и... job'а появилась!\n",
    "\n",
    "В Spark'е сть 2 типа операций над RDD:\n",
    "* [трансформации](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations). Преобразуют RDD в новое RDD.\n",
    "* [действия](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions). Преобразуют RDD в обычный объект.\n",
    "\n",
    "Трансформации выполяются **лениво**. При вызове трансформации достраивается граф вычислений и больше ничего не происходит. \n",
    "\n",
    "Реальное выполнение графа происходит при вызове Action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## WordCount на Spark\n",
    "\n",
    "Мы уже прочитали данные, теперь попробуем посчитать на них WordCount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# строим граф вычислений\n",
    "rdd = sc.textFile(\"/data/griboedov\")\n",
    "rdd = rdd.map(lambda x: x.strip().lower()) # приводим к нижнему регистру\n",
    "rdd = rdd.flatMap(lambda x: x.split(\" \")) # выделяем слова\n",
    "rdd = rdd.map(lambda x: (x, 1))  # собираем пары (word, 1)\n",
    "rdd = rdd.reduceByKey(lambda a, b: a + b) # суммируем \"1\" с одинаковыми ключами\n",
    "rdd = rdd.sortBy(lambda a: -a[1]) # сортируем по кол-ву встречаемости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rdd.take(10) # Action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Типы трансформаций в Spark\n",
    "\n",
    "![Image](images/stages.png)\n",
    "(https://www.slideshare.net/LisaHua/spark-overview-37479609)\n",
    "\n",
    "* Часть трансформаций (map, flatmap, ...) обрабатывает партиции независимо. Такие трансформации называются *narrow*. \n",
    "* reduce, sortBy аггрегируют данные и используют передачу по сети. Они называются *wide*. \n",
    "   * Wide-трансформации могут менять кол-во партиций.\n",
    "   * По wide-трансформациям происходит деление job'ы на Stages.\n",
    "\n",
    "Stage тоже делится на task'и. 1 task выполняется для одной партиции.\n",
    "\n",
    "**Итак: Task << Stage << Job << Application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "В Spark есть возможность вывести план job'ы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(rdd.toDebugString().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Видим всего 3 трансформации. Где все остальные?\n",
    "\n",
    "Spark написан на Scala, которая под капотом использует JVM. Чтоб делать вычисления в Python, нужно вытаскивать данные из JVM. А потом возвращаться обратно. Получаем OverHead на сериализацию-десериализацию. Чтоб overhead'ов было меньше, схлопываем узкие трансформации в одну."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Весь пример целиком:** `/pd-seminars/seminar-14/examples/00-griboedov-wordcount/main.py`\n",
    "\n",
    "Запустим с помощью `./run.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задача 1.\n",
    "\n",
    "> При подсчёте отсеять пунктуацию и слова короче 3 символов. \n",
    "При фильтрации можно использовать регулярку: `re.sub(u\"\\\\W+\", \" \", x.strip(), flags=re.U)`.\n",
    "\n",
    "**Решение:** `/pd-seminars/seminar-14/examples/01-task1/main.py`\n",
    "\n",
    "### Задача 2.\n",
    "\n",
    "> Считать только имена собственные. Именами собственными в данном случае будем считать такие слова, у которых 1-я буква заглавная, остальные - прописные.\n",
    "\n",
    "**Решение:** `/pd-seminars/seminar-14/examples/02-task2/main.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Аккумуляторы\n",
    "\n",
    "Аналоги счётчиков в Hadoop. \n",
    "* Используется для легковесной аггрегации (без `reduceByKey` и дополнительных shuffle'ов)\n",
    "* Если аккумулятор используется в трансформациях, то нельзя гарантировать консистентность (мы можем с помощью action'a вызвать DAG несколько раз). Можно использовать в `foreach()`.\n",
    "\n",
    "**Объявление:** `cnt = sc.accumulator(start_val)`\n",
    "\n",
    "**Использование:** \n",
    "   * Inline: `foreach(lambda x: cnt.add(x))`\n",
    "   * Или же, с помощью своей функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def count_with_conditions(x):\n",
    "    global cnt\n",
    "    if ...:\n",
    "        cnt += 1\n",
    "\n",
    "rdd.foreach(lambda x: count_with_conditions(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Получение результата:** `cnt.value`\n",
    "\n",
    "Подробнее в [документации](http://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задача 3.\n",
    "\n",
    "> Переделайте задача 2 так, чтоб кол-во имён собственных вычислялось с помощью аккумулятора.\n",
    "\n",
    "**Решение:** `/pd-seminars/seminar-14/examples/03-task3/main.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Broadcast-переменные\n",
    "\n",
    "Аналог DistributedCache в Hadoop. Обычно используется когда мы хотим в спарке сделать Map-side join (т.е. имеется 2 датасета: 1 маленький, который и добавляем в broadcast, другой большой)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "br_cast = sc.broadcast([\"hadoop\", \"hive\", \"spark\", 'zookeeper', 'kafka']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "br_cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "br_cast.value[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Кеширование\n",
    "\n",
    "При перезапуске Action, пересчитывается весь граф вычислений. Это логично т.к. в трансформациях ничего не вычисляется. Полезно это тем, что если за время работы задачи данные обновились (дополнились), нам достаточно просто перевызвать Action.\n",
    "\n",
    "Но если данные не меняются (например, при отладке), такой пересчёт даёт Overhead. Можно **закешировать** часть pipeline. Тогда при след. вызове Action, RDD считается с кеша и пересчёт начнётся с того места, где было кеширование. В History UI все Stage перед этим будут помечены \"Skipped\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/griboedov\")\n",
    "rdd = rdd.map(lambda x: x.strip().lower())\n",
    "rdd = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "rdd = rdd.map(lambda x: (x, 1)).cache() # тут всё хорошо работает, кешируем\n",
    "rdd = rdd.reduceByKey(lambda a, b: a + b) # а тут хотим отладить, поэтому будут перезапуски\n",
    "rdd = rdd.sortBy(lambda a: -a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Кешировать можно с помощью двух операций:\n",
    "* `cache()`\n",
    "* `persist(storage_level)`\n",
    "\n",
    "В `persist()` можно указать [StorageLevel](https://spark.apache.org/docs/2.1.2/api/python/_modules/pyspark/storagelevel.html), т.е. на какой носитель кешируем. Можем закешировать в диск, в память, на диск и / или память на несколько нод... или дать возможность Spark'у решить самому (на основе объёма кеша).\n",
    "\n",
    "`cache()` - это простой вариант `persist()`, когда кешируем только в RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Практические задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "В hdfs в папке `/data/access_logs/big_log` лежит лог в формате\n",
    "\n",
    "* IP-адрес пользователя (`195.206.123.39`),\n",
    "* Далее идут два неиспользуемых в нашем случае поля (`-` и `-`),\n",
    "* Время запроса (`[24/Sep/2015:12:32:53 +0400]`),\n",
    "* Строка запроса (`\"GET /id18222 HTTP/1.1\"`),\n",
    "* HTTP-код ответа (`200`),\n",
    "* Размер ответа (`10703`),\n",
    "* Реферер (источник перехода; `\"http://bing.com/\"`),\n",
    "* Идентификационная строка браузера (User-Agent; `\"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"`).\n",
    "\n",
    "Созданы несколько семплов данных разного размера:\n",
    "```\n",
    "3.4 G    10.2 G   /data/access_logs/big_log\n",
    "17.6 M   52.7 M   /data/access_logs/big_log_10000\n",
    "175.4 M  526.2 M  /data/access_logs/big_log_100000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Пример парсинга логов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET = \"/data/access_logs/big_log_10000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime as dt\n",
    "\n",
    "log_format = re.compile( \n",
    "    r\"(?P<host>[\\d\\.]+)\\s\" \n",
    "    r\"(?P<identity>\\S*)\\s\" \n",
    "    r\"(?P<user>\\S*)\\s\"\n",
    "    r\"\\[(?P<time>.*?)\\]\\s\"\n",
    "    r'\"(?P<request>.*?)\"\\s'\n",
    "    r\"(?P<status>\\d+)\\s\"\n",
    "    r\"(?P<bytes>\\S*)\\s\"\n",
    "    r'\"(?P<referer>.*?)\"\\s'\n",
    "    r'\"(?P<user_agent>.*?)\"\\s*'\n",
    ")\n",
    "\n",
    "def parseLine(line):\n",
    "    match = log_format.match(line)\n",
    "    if not match:\n",
    "        return (\"\", \"\", \"\", \"\", \"\", \"\", \"\" ,\"\", \"\")\n",
    "\n",
    "    request = match.group('request').split()\n",
    "    return (match.group('host'), match.group('time').split()[0], \\\n",
    "       request[0], request[1], match.group('status'), match.group('bytes'), \\\n",
    "        match.group('referer'), match.group('user_agent'),\n",
    "        dt.strptime(match.group('time').split()[0], '%d/%b/%Y:%H:%M:%S').hour)\n",
    "\n",
    "\n",
    "lines = sc.textFile(DATASET)\n",
    "parsed_logs = lines.map(parseLine).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2й вариант парсинга - с помощью namedtuple\n",
    "\n",
    "```python\n",
    "LogItem = namedtuple(\"LogItem\", \n",
    "                     [\"host\", \"time\", \"method\", \"path\", \"status\", \"length\", \"referer\", \"user_agent\", \"hour\"])\n",
    "\n",
    "def parseLine(line):\n",
    "    match = log_format.match(line)\n",
    "    if not match:\n",
    "        return LogItem(\"\", \"\", \"\", \"\", \"\", \"\", \"\" ,\"\", \"\")\n",
    "\n",
    "    request = match.group('request').split()\n",
    "    return LogItem(\n",
    "        host=match.group('host'),\n",
    "        time=match.group('time').split()[0],\n",
    "        method=request[0],\n",
    "        path=request[1],\n",
    "        status=match.group('status'),\n",
    "        length=match.group('bytes'),\n",
    "        referer=match.group('referer'),\n",
    "        user_agent=match.group('user_agent'),\n",
    "        hour=dt.strptime(match.group('time').split()[0],'%d/%b/%Y:%H:%M:%S').hour\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Распарсили, получили RDD, закешировали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parsed_logs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "log_format = re.compile(\n",
    "    r\"(?P<host>[\\d\\.]+)\\s\"\n",
    "    r\"(?P<identity>\\S*)\\s\"\n",
    "    r\"(?P<user>\\S*)\\s\"\n",
    "    r\"\\[(?P<time>.*?)\\]\\s\"\n",
    "    r'\"(?P<request>.*?)\"\\s'\n",
    "    r\"(?P<status>\\d+)\\s\"\n",
    "    r\"(?P<bytes>\\S*)\\s\"\n",
    "    r'\"(?P<referer>.*?)\"\\s'\n",
    "    r'\"(?P<user_agent>.*?)\"\\s*'\n",
    ")\n",
    "\n",
    "\n",
    "def parseLine(line):\n",
    "    match = log_format.match(line)\n",
    "    if not match:\n",
    "        return (\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\")\n",
    "\n",
    "    request = match.group('request').split()\n",
    "    return (match.group('host'), match.group('time').split()[0],\n",
    "        request[0], request[1], match.group('status'), int(match.group('bytes')),\n",
    "        match.group('referer'), match.group('user_agent'),\n",
    "        dt.strptime(match.group('time').split()[0], '%d/%b/%Y:%H:%M:%S').hour)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark_session = SparkSession.builder.master(\"yarn\").appName(\"501 df\").config(\"spark.ui.port\", \"18089\").getOrCreate()\n",
    "    lines = spark_session.sparkContext.textFile(\"%s\" % sys.argv[1])\n",
    "    parts = lines.map(parseLine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задача 4.\n",
    "> Напишите программу, выводящую на экран TOP5 ip адресов, в которых содержится хотя бы одна цифра 4, с наибольшим количеством посещений.\n",
    "Каждая строка результата должна содержать IP адрес и число посещений, разделенные табуляцией, строки должны быть упорядочены по числу посещений по убыванию, например:\n",
    "```\n",
    "195.206.123.39<TAB>40\n",
    "196.206.123.40<TAB>39\n",
    "191.206.123.41<TAB>38\n",
    "175.206.123.42<TAB>37\n",
    "195.236.123.43<TAB>36\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Решение:** `/pd-seminars/seminar-14/examples/04-parsedlogs-top5/main.py`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задача 5.\n",
    ">  Напишите программу, выводящую на экран суммарное распределение количества посетителей по часам (для каждого часа в сутках вывести количество посетителей, пришедших в этот час). Id посетителя = ip + user_agent.\n",
    "Результат должен содержать час в сутках и число посетителей, разделенные табом и упорядоченные по часам. Например:\n",
    "```\n",
    "0<tab>10\n",
    "1<tab>10\n",
    "2<tab>10\n",
    "…..\n",
    "21<tab>30\n",
    "22<tab>20\n",
    "23<tab>10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Решение:** `/pd-seminars/seminar-14/examples/05-parsedlogs-distribution/main.py`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}